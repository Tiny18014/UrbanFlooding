{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 2: GNN + LSTM (Recommended)\n",
    "\n",
    "## OBJECTIVE: Build a Graph Neural Network + LSTM hybrid model for spatiotemporal flood forecasting\n",
    "\n",
    "### TASK DESCRIPTION:\n",
    "Implement a state-of-the-art deep learning model that combines:\n",
    "1. Graph Neural Networks (GNN) for spatial structure\n",
    "2. LSTM/GRU for temporal dynamics\n",
    "3. Separate encoders for 1D and 2D nodes\n",
    "4. 1D-2D coupling mechanism\n",
    "5. Multi-scale rainfall feature engineering (from research paper)\n",
    "\n",
    "### ARCHITECTURE SPECIFICATION:\n",
    "\n",
    "1. FEATURE ENGINEERING (Based on Shenzhen Paper):\n",
    "   Multi-Temporal Rainfall Aggregation:\n",
    "   - Rolling sum: 15min, 30min, 1h, 3h, 6h\n",
    "   - Rolling max: 1h, 3h\n",
    "   - Cumulative rainfall from event start\n",
    "   - Rainfall rate (first derivative)\n",
    "   \n",
    "   Static features:\n",
    "   - Node embeddings (learned)\n",
    "   - Elevation, area, roughness (normalized)\n",
    "   - Storage capacity (engineered)\n",
    "   \n",
    "   Temporal features:\n",
    "   - Water level lags: t-1, t-2, t-3\n",
    "   - Velocity (if available)\n",
    "   - Time since event start\n",
    "\n",
    "2. MODEL ARCHITECTURE:\n",
    "\n",
    "   a) Separate 1D and 2D Encoders:\n",
    "   \n",
    "   1D Encoder (High Capacity):\n",
    "   - Input: [17 nodes × features]\n",
    "   - Graph Attention Network (GAT): 4 layers, 8 heads, 512 hidden\n",
    "   - Node features → Spatial embedding\n",
    "   \n",
    "   2D Encoder (Efficient):\n",
    "   - Input: [3716 nodes × features]\n",
    "   - Graph Convolutional Network (GCN): 4 layers, 256 hidden\n",
    "   - OR ChebConv for large graphs\n",
    "   - Node features → Spatial embedding\n",
    "   \n",
    "   b) Coupling Layer:\n",
    "   - Use 1d2d_connections.csv to create bipartite edges\n",
    "   - Message passing from 1D to 2D: aggregation of 1D embeddings to connected 2D nodes\n",
    "   - Message passing from 2D to 1D: aggregation of 2D embeddings to connected 1D nodes\n",
    "   \n",
    "   c) Temporal Module:\n",
    "   - Input: Concatenate [spatial_embedding, rainfall_features, time_features]\n",
    "   - LSTM: 3 layers, 512 hidden (for 1D), 256 hidden (for 2D)\n",
    "   - Bidirectional: False (causal prediction only)\n",
    "   - Dropout: 0.3\n",
    "   \n",
    "   d) Decoder:\n",
    "   - 1D Decoder: MLP [512 → 256 → 128 → 1]\n",
    "   - 2D Decoder: MLP [256 → 128 → 1]\n",
    "   - Output: Predicted water level at t+1\n",
    "\n",
    "3. TRAINING CONFIGURATION:\n",
    "\n",
    "   Loss Function:\n",
    "   ```python\n",
    "   def standardized_rmse_loss(pred, target, model_id, node_types):\n",
    "       std_1d = 16.88 if model_id==1 else 3.19\n",
    "       std_2d = 14.38 if model_id==1 else 2.73\n",
    "       \n",
    "       rmse_1d = sqrt(mean((pred[1D] - target[1D])^2)) / std_1d\n",
    "       rmse_2d = sqrt(mean((pred[2D] - target[2D])^2)) / std_2d\n",
    "       \n",
    "       return (rmse_1d + rmse_2d) / 2  # Equal weighting\n",
    "   ```\n",
    "   \n",
    "   Multi-Step Loss (for long sequences):\n",
    "   ```python\n",
    "   loss_total = 0\n",
    "   for k in [1, 2, 3, 5, 10]:  # Predict 1, 2, 3, 5, 10 steps ahead\n",
    "       pred_k = model.predict_k_ahead(x, k)\n",
    "       loss_k = standardized_rmse_loss(pred_k, y[k])\n",
    "       loss_total += (1/k) * loss_k  # Weight closer predictions more\n",
    "   ```\n",
    "   \n",
    "   Optimizer:\n",
    "   - AdamW with weight decay 1e-4\n",
    "   - Learning rate: 1e-3 for 1D encoder, 5e-4 for 2D encoder\n",
    "   - Cosine annealing schedule with warm restarts\n",
    "   \n",
    "   Regularization:\n",
    "   - Dropout: 0.3-0.4\n",
    "   - Gradient clipping: max_norm=1.0\n",
    "   - Early stopping: patience=15 epochs\n",
    "   - L2 weight decay: 1e-4\n",
    "\n",
    "4. AUTOREGRESSIVE FORECASTING:\n",
    "\n",
    "   Teacher Forcing Schedule:\n",
    "   - Epochs 1-20: 90% teacher forcing (use ground truth)\n",
    "   - Epochs 21-40: 70% teacher forcing\n",
    "   - Epochs 41-60: 50% teacher forcing\n",
    "   - Epochs 61+: 30% teacher forcing\n",
    "   \n",
    "   Scheduled Sampling:\n",
    "   ```python\n",
    "   if epoch < 20:\n",
    "       teacher_forcing_ratio = 0.9\n",
    "   elif epoch < 40:\n",
    "       teacher_forcing_ratio = 0.7\n",
    "   elif epoch < 60:\n",
    "       teacher_forcing_ratio = 0.5\n",
    "   else:\n",
    "       teacher_forcing_ratio = 0.3\n",
    "   \n",
    "   if random.random() < teacher_forcing_ratio:\n",
    "       input_t = ground_truth[t]\n",
    "   else:\n",
    "       input_t = prediction[t-1]\n",
    "   ```\n",
    "   \n",
    "   Noise Injection (reduce error accumulation):\n",
    "   ```python\n",
    "   if training:\n",
    "       prediction += torch.randn_like(prediction) * 0.01 * std\n",
    "   ```\n",
    "\n",
    "5. DATA PIPELINE:\n",
    "\n",
    "   Dataset Structure:\n",
    "   - Train: Events 1-54 (80%)\n",
    "   - Validation: Events 55-68 (20%)\n",
    "   - Stratify by event clusters from Notebook 05\n",
    "   \n",
    "   Batching:\n",
    "   - Batch size: 16 sequences (or full events)\n",
    "   - Sequence length: Variable (94-205), use padding + masking\n",
    "   - Data augmentation: Add Gaussian noise to inputs (σ=0.01)\n",
    "   \n",
    "   Normalization:\n",
    "   - Water levels: Standardize per model (use computed μ, σ)\n",
    "   - Rainfall: MinMax to [0, 1]\n",
    "   - Static features: StandardScaler\n",
    "\n",
    "6. CROSS-VALIDATION:\n",
    "\n",
    "   5-Fold Event-Based CV:\n",
    "   - Ensure each fold has events from all 4 clusters\n",
    "   - Don't split individual events\n",
    "   - Validate on full sequences (not truncated)\n",
    "\n",
    "### IMPLEMENTATION STEPS:\n",
    "\n",
    "Week 1 (Days 1-7):\n",
    "- Day 1: Implement multi-scale rainfall feature engineering\n",
    "- Day 2: Build data loader with PyTorch Geometric\n",
    "- Day 3: Implement GNN encoders (GAT for 1D, GCN for 2D)\n",
    "- Day 4: Add coupling layer (1D ↔ 2D message passing)\n",
    "- Day 5: Integrate LSTM temporal module\n",
    "- Day 6: Implement standardized RMSE loss\n",
    "- Day 7: Train first baseline, validate, debug\n",
    "\n",
    "Week 2 (Days 8-14):\n",
    "- Day 8: Add multi-step loss\n",
    "- Day 9: Implement teacher forcing schedule\n",
    "- Day 10: Add noise injection for robustness\n",
    "- Day 11: Hyperparameter tuning (grid search)\n",
    "- Day 12: Cross-validation setup\n",
    "- Day 13: Train on all folds\n",
    "- Day 14: Ensemble fold models\n",
    "\n",
    "Week 3 (Days 15-21):\n",
    "- Day 15: Train on Model 2 data\n",
    "- Day 16: Final hyperparameter optimization\n",
    "- Day 17: Error analysis (where does model fail?)\n",
    "- Day 18: Model refinement based on analysis\n",
    "- Day 19: Generate test predictions\n",
    "- Day 20: Validate submission format\n",
    "- Day 21: Final submission + documentation\n",
    "\n",
    "### CODE SKELETON:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "class HybridGNN_LSTM(nn.Module):\n",
    "    def __init__(self, n_1d_nodes=17, n_2d_nodes=3716, \n",
    "                 static_dim_1d=7, static_dim_2d=10,\n",
    "                 hidden_1d=512, hidden_2d=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Node embeddings\n",
    "        self.node_embed_1d = nn.Embedding(n_1d_nodes, 64)\n",
    "        self.node_embed_2d = nn.Embedding(n_2d_nodes, 32)\n",
    "        \n",
    "        # 1D Encoder (GAT)\n",
    "        self.gat1d_1 = GATConv(static_dim_1d+64, hidden_1d, heads=8, dropout=0.3)\n",
    "        self.gat1d_2 = GATConv(hidden_1d*8, hidden_1d, heads=8, dropout=0.3)\n",
    "        self.gat1d_3 = GATConv(hidden_1d*8, hidden_1d, heads=4, dropout=0.3)\n",
    "        \n",
    "        # 2D Encoder (GCN)\n",
    "        self.gcn2d_1 = GCNConv(static_dim_2d+32, hidden_2d)\n",
    "        self.gcn2d_2 = GCNConv(hidden_2d, hidden_2d)\n",
    "        self.gcn2d_3 = GCNConv(hidden_2d, hidden_2d)\n",
    "        \n",
    "        # Coupling layer (bipartite graph)\n",
    "        self.coupling_1d_to_2d = GCNConv(hidden_1d*4, hidden_2d)\n",
    "        self.coupling_2d_to_1d = GCNConv(hidden_2d, hidden_1d)\n",
    "        \n",
    "        # Temporal module\n",
    "        self.lstm_1d = nn.LSTM(hidden_1d + 10, hidden_1d, \n",
    "                               num_layers=3, dropout=0.3, batch_first=True)\n",
    "        self.lstm_2d = nn.LSTM(hidden_2d + 10, hidden_2d, \n",
    "                               num_layers=3, dropout=0.3, batch_first=True)\n",
    "        \n",
    "        # Decoders\n",
    "        self.decoder_1d = nn.Sequential(\n",
    "            nn.Linear(hidden_1d, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.decoder_2d = nn.Sequential(\n",
    "            nn.Linear(hidden_2d, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def encode_1d(self, x, edge_index):\n",
    "        # Placeholder for encoding logic\n",
    "        return x # dummy\n",
    "\n",
    "    def encode_2d(self, x, edge_index):\n",
    "        # Placeholder for encoding logic\n",
    "        return x # dummy\n",
    "\n",
    "    def couple(self, x_1d, x_2d, coupling_edges):\n",
    "        # Placeholder for coupling logic\n",
    "        return x_1d, x_2d # dummy\n",
    "\n",
    "    def forward(self, data, rainfall_features, teacher_forcing_ratio=0.5):\n",
    "        # Encode spatial structure\n",
    "        x_1d = self.encode_1d(data['x_1d'], data['edge_index_1d'])\n",
    "        x_2d = self.encode_2d(data['x_2d'], data['edge_index_2d'])\n",
    "        \n",
    "        # Coupling\n",
    "        x_1d, x_2d = self.couple(x_1d, x_2d, data['coupling_edges'])\n",
    "        \n",
    "        # Temporal processing (autoregressive)\n",
    "        seq_len = rainfall_features.shape[1]\n",
    "        predictions_1d = []\n",
    "        predictions_2d = []\n",
    "        \n",
    "        h_1d, c_1d = None, None\n",
    "        h_2d, c_2d = None, None\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Teacher forcing\n",
    "            if t == 0 or random.random() > teacher_forcing_ratio:\n",
    "                input_1d = x_1d\n",
    "                input_2d = x_2d\n",
    "            else:\n",
    "                # Use previous prediction\n",
    "                input_1d = predictions_1d[-1]\n",
    "                input_2d = predictions_2d[-1]\n",
    "            \n",
    "            # Concatenate with rainfall features\n",
    "            lstm_input_1d = torch.cat([input_1d, rainfall_features[:, t, :10]], dim=-1)\n",
    "            lstm_input_2d = torch.cat([input_2d, rainfall_features[:, t, :10]], dim=-1)\n",
    "            \n",
    "            # LSTM step\n",
    "            out_1d, (h_1d, c_1d) = self.lstm_1d(lstm_input_1d.unsqueeze(1), (h_1d, c_1d))\n",
    "            out_2d, (h_2d, c_2d) = self.lstm_2d(lstm_input_2d.unsqueeze(1), (h_2d, c_2d))\n",
    "            \n",
    "            # Decode\n",
    "            pred_1d = self.decoder_1d(out_1d.squeeze(1))\n",
    "            pred_2d = self.decoder_2d(out_2d.squeeze(1))\n",
    "            \n",
    "            predictions_1d.append(pred_1d)\n",
    "            predictions_2d.append(pred_2d)\n",
    "        \n",
    "        return torch.stack(predictions_1d, dim=1), torch.stack(predictions_2d, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
